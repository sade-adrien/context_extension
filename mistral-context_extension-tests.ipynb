{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, Trainer, TrainingArguments, logging\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from huggingface_hub import notebook_login\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "from tools import *\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e91aecd43d33485a88599f9219831dea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c8b1f3bfdd4137ac49fd511f464845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276e1d1acafd40a687d041921cf7a609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d092ead431d54dd283868e09b6c01101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92240079658c4a36b085ecb761a42849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\"\n",
    "checkpoint = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "#config.update({'sliding_window' : 8_192}) \n",
    "#config.update({'rope_scaling' : {\"type\": \"yarn\",\n",
    "#                                 \"factor\": 4, \n",
    "#                                 \"original_max_position_embeddings\": 8192,\n",
    "#                                 \"finetuned\": True,\n",
    "#                                }})  \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast = False, revision = 'main')\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint,\n",
    "                                            low_cpu_mem_usage = True,\n",
    "                                            torch_dtype = torch.float16,\n",
    "                                            revision = 'main',\n",
    "                                            device_map = 'auto',\n",
    "                                            use_flash_attention_2 = True,\n",
    "                                            config = config,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r_default = 8\n",
    "lora_alpha_default = 32\n",
    "lora_dropout_default = 0.05\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "        r=lora_r_default, \n",
    "        lora_alpha=lora_alpha_default, \n",
    "        lora_dropout=lora_dropout_default,\n",
    "        bias=\"none\", \n",
    "        task_type=\"CAUSAL_LM\",  \n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "        )\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136,056,832 parameters were loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "path = './model_weights/Mistral-7B-v0.1-context_extension-stage3bis/checkpoint_2400.pt'\n",
    "\n",
    "def load_weights(model, path):\n",
    "    saved_weights = torch.load(path)\n",
    "    param_count = 0\n",
    "    for key, val in saved_weights['model_state_dict'].items():\n",
    "        for name, param in model.named_parameters():\n",
    "            if key == name:\n",
    "                param.data = val.data\n",
    "                param_count += val.numel()\n",
    "                break\n",
    "    \n",
    "    print(f'{param_count:,} parameters were loaded successfully.')\n",
    "\n",
    "load_weights(model, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,p in model.named_parameters():\n",
    "    p.data = p.data.to(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def loading_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.astype(str)\n",
    "    df = df.loc[df[\"Value\"] != \"Value\"]\n",
    "\n",
    "    content = \"\"\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.iloc[i]['Value'] == \"nan\":\n",
    "            content += \"\\n\" + df.iloc[i]['Attribute'] + \" \" + \"None\"\n",
    "        else:\n",
    "            content += \"\\n\" + df.iloc[i]['Attribute'] + \" \" + df.iloc[i]['Value']\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Changes made in the Updated Report:\n",
      "\n",
      "* Date of patient visit: Changed from 8/24/2022 to 9/12/2023\n",
      "* Time of patient visit: Changed from 14:30 to 15:30\n",
      "* Reason for visit: Changed from Annual checkup to Pre-visit for surgery after Achilles tendon rupture during tennis game\n",
      "* Name of healthcare provider: Changed from Dr. Michelle Clovis to Dr. Michael Frank\n",
      "* Clinic/Department visited: Changed from 2134 to 2324\n",
      "* Past illnesses and hospitalizations: Added Incident2: 9/12/2023 serious sprain of the lateral ankle ligament on right foot with Achilles tendon rupture\n",
      "* Plan for further tests: Added MRI 3 days after surgery\n",
      "* Referrals to specialists: Added Physical Therapist, sports medicine: Dr. Max Edberg\n",
      "* Patient instructions - life style: Added Immobilization of ankle: 3 weeks pointed foot, 3 weeks at 90 degrees, ankle rehabilitation: 3 weeks, no walking on foot for 7 weeks minimum\n",
      "* Billing code for services rendered: Changed from J48372 to J48392\n",
      "* Cost detail for patient and insurance: Changed from $50 (deductable) /$1150 total to $50 (deductable) /$1150 total</s>\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "prompt = f\"\"\"[INST] Record: [ {loading_data('../medical_demo/EX2.csv')} ]\n",
    "Updated Record: [ {loading_data('../medical_demo/EX2_update.csv')} ] \n",
    "Here are an original medical record and its update for a patient. Using bullet points, report the changes that have been made in the Updated Report, ONLY if there was an actual change. [/INST]\"\"\"\n",
    "inputs = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=900)\n",
    "    print(tokenizer.decode(outputs[0, inputs['input_ids'].shape[1]:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesis = r\"\"\"\\chapter{Introduction}\n",
    "\\label{chap:introduction}\n",
    "\n",
    "The year 2023 is marked as the year of Generative Artificial Intelligence. The development of such models actually started a few years back but some major late advancements, including the public releases of OpenAI's DALL·E and ChatGPT in late 2022 or the leakage of Meta's LLaMA weights in early 2023, created a public excitement for these models. This enthusiasm is only the natural result of the - now proven - capabilities of such models and what they could bring to our society in a near future. If most applications of the early stages development sounded relatively shallow - for instance writing a cover letter with a Large Language Model (LLM) or creating a new logo with a text-to-image model - they only give a glimpse of what will be possible in the next couple of years. For instance, automated speech recognition and transcription could be an enormous workload saver for doctors in the US who have to maintain a thorough record of their discussions with patients - and they could even feed this transcription to a medical LLM providing a first draft of a patient analysis.\n",
    "\n",
    "\\ \\\\\n",
    "Both the private industry and the open-source community have been working to unlock the potential of Generative AI models and so far they have been highly successful in their doings. The growing collaboration between Big Tech companies and the open-source community enabled for discoveries every other days. One major obstacle in this field of research is the computational resource required to train Generative AI models. Some models developed have hundreds of billions of parameters and requisite a tremendous amount of computations and thousands of GPUs to be effectively trained. This is both costly (up to several millions of dollars) and time-costly (up to several months). Such research is obviously inaccessible to most companies, universities and individuals and has to be performed by FAANG companies. However, given the buzz in the open-source community around this field, techniques have emerged to make possible reduced-cost training and empower smaller entities.\n",
    "\n",
    "\n",
    "Amongst those models, Large Language Models have received very special attention. A LLM is designed to process Natural Language (NL) - i.e. regular written text. It can have a specific task such as sentiment classification (e.g. determining if a tweet is angry or happy), machine translation (e.g. translating English sentences to French) or text generation (e.g. predicting the answer to a question). Their large range of possible tasks, their high potential and their easiness to use for the public, make them extremely attractive to the industry. This is especially true since the development of research in the field of small LLMs. With advanced implementation and curated data, some teams have developed small-size general-purpose LLMs (around 7 billion parameters) and were able to outperform mid-size models (up to 30 billion parameters) and even match full-scale models (70 billion parameters and higher) on specific tasks.\n",
    "\n",
    "\\ \\\\\n",
    "If LLMs as a whole receive a lot of attention, there is one specific subcategory of LLMs that is even more targeted by this world effort. General-purpose LLMs, or Foundation Models, are designed to learn and retain a tremendous volume of knowledge. The objective is to build a giant database of our society and knowledge as humans. In a way, they are designed to replicate the internet and to this end they are mostly trained with billions of documents taken off of the web. Retaining such a volume of information is challenging, thus Foundation Models are usually extremely large. Their sizes range from a few billions of parameters all the way up to half a trillion of parameters. Once trained and released, they find their utility in downstream applications. With an additional small fine-tuning they can be used for various tasks with extreme accuracy. Fine-tuning is the process of continuing the training of a Pre-Trained Model to specialize it to a specific downstream task. An example is Foundation Model GPT-3 being fine-tuned for Question-Answering tasks into the user-accessible chatbot ChatGPT. Fine-tuning presents the advantage of building task-specific models at a low cost while leveraging the knowledge and capabilities of Pre-Trained Models. The process is as simple as re-using the weights of the Pre-Trained Model during initialization for the new training. More precisely, this is called Full Fine-Tuning as all parameters are updated regardless. Nevertheless, because of the large size of Foundation Models, it can be still inaccessible to smaller entities to fine-tune such models. That is why, the scientific community has developed a large palette of techniques reducing the number of trainable parameters during the fine-tuning phase. This is known as Reduced-Parameters Fine-Tuning.\n",
    "\n",
    "\\ \\\\\n",
    "In this report, we first introduce Large Language Models and their architectures by diving in them at a technical level. We then present a range of machine learning methods that helped us build our knowledge before contributing ourselves. Using these previous parts, we lay out the architecture developed for SteloCoder, the training process and the data used, along with the results obtained. Finally, we acknowledge the limitations of our work and propose several areas of improvement for future work.\n",
    "\n",
    "The first two chapters can be regarded as a large literature review of what we have learned and tried applying while preparing our SteloCoder model. They present a range of techniques that we carefully reviewed and selected to build the perfect model - in the limits of the constraints we faced. Please note that the construction of our model in the last two chapters entirely relies on this and cannot exist without this prior knowledge\\footnote{We could re-structure this report to prevent it from having a large block of literature review by including these whenever they rise up, but this would result in a much less intelligible paper. We suggest the technically savvy readers to skip chapters \\ref{chap:llm} and \\ref{chap:back_work}.}.\n",
    "\n",
    "\\chapter{Large Language Models}\n",
    "\n",
    "\\section{Objective}\n",
    "\n",
    "Large Language Models are used for various NLP tasks. They usually take a chunk of text as input and can have different types of output. They can be used for sequence/token classification for instance, or for sequence-to-sequence tasks. The latter regroups tasks like text summarization, translation or question/answering\\footnote{Actually, sequence-to-sequence designates an even larger range of tasks as it is not restrained to NLP (e.g. audio sequence to text transcription).}. Below, we formalize the mathematical objective of a sequence-to-sequence models as it is representative of LLMs' tasks and can be generalized easily.\n",
    "\n",
    "Once this distribution is computed, we can select a vector output, for example, using the max-probability output (greedy search). More complex methods \\citep{generating1, generating2, generating3} allow to steer, stabilize and enhance the creativity of the output regardless of the actual model (distribution). This has been an active field of research and we leave it here as this not the main subject of this report. Yet, one should really appreciate the importance of the text generation technique chosen as it greatly impacts the Language Modeling (LM) behavior of the model\\footnote{The question is addressed much more easily when it comes to other tasks such as text classification.}.\n",
    "\n",
    "\\section{Architecture}\n",
    "\\subsection{Transformer-based}\n",
    "\n",
    "In 2017, \\cite{attention} released the Transformer architecture. They revolutionized the field of NLP as scientists would now exclusively adopt this architecture. It has also impacted other fields more recently, such as Computer Vision in which Transformers have proven efficient \\citep{cv_transformer, swin_transformer}. This can be explained by several factors, the most important ones being the capability of the attention mechanism to capture short and long-range dependencies, and the structure that allows easy parallelization of operations taking advantage of multiple GPUs to exceedingly fasten the training.\n",
    "\n",
    "\\ \\\\\n",
    "On a high-level, the Transformer-based LLM architecture can be broken down in three parts: the embeddings, the actual transformer and the model head. \n",
    "\n",
    "\\subsubsection{(Encodings \\&) Embeddings}\n",
    "\n",
    "As mentioned before, the LLM framework suggests a text input. However, it is not possible to perform computations directly on chunks of text. The first problem that needs to be addressed is then the mapping of a Natural Language text to a mathematical input. This is done in two\\footnote{three?} steps.\n",
    "\n",
    "\\ \\\\\n",
    "Before being fed to the model, the input text is passed through a tokenizer. A tokenizer is usually itself the result of a trained machine learning model that converts a sequence of characters to a sequence of non-negative integers - we call this new representation of the input \"the encodings\". Formally, this first step consists in mapping the string $\\mathbf{x}_\\texttt{NL}$ from the $\\mathbf{NL}$ language set to an integer-vector representation.\n",
    "\n",
    "There are various levels of tokenization: character-level (aka byte-level) which converts each single character to a unique non-negative integer \\citep{byte-level1, byte-level2}, word-level which converts each word present in the training corpus to a unique non-negative integer \\citep{transfoXL} or subword-level which has an intermediary level of tokenization \\citep{bert, gpt2}. \n",
    "\n",
    "Using a character-level tokenization allows a great flexibility in the model generation as it processes one character at a time, and it limits the vocabulary size as there are only so many letters and characters in a NL text. However, it limits the understanding depth of the language as word semantic is ignored from the model. Additionally, word-level tokenization has a better global computational efficiency because sequences of characters are processed and generated simultaneously. For a $5$-character word we would run the model once, against 5 times for a byte-level tokenization. However, there can be a humongous number of words in a corpus of texts, and while some of them will be very rare, this will directly impact the model size. \n",
    "\n",
    "There is obviously a trade-off to find between these methods and that is why subword-level tokenizers were introduced and are the most common tokenization technique. There are plenty of ways to build those using fusion techniques, prefix/suffix separation or frequency based methods for instance \\citep{NMT-tokenization, googleNMT-tokenization}. But we are not going to dive deeper in tokenization as this is a whole field of research by itself and not the heart of our study. Again, note that this is an important part of NLP and it can have a great impact on the model's behavior. A tokenizer is always associated and released along with a model as the latter is trained for specific encodings. The tokenizer is not per say part of the LLM but it is intrinsically linked and always used along.\n",
    "In what follows we refer to the the elements of the encoded vector as \"tokens\" regardless of the level of tokenization. \n",
    "\n",
    "\\ \\\\\n",
    "At this point, we have a mathematical representation of the input text. These numbers could technically be used for computations in the model. Nevertheless, the encoding process comes with a great issue: it arbitrarily associates a token to a unique number, which naturally orders the tokens. We could imagine trying to make some sense when building this order and ensure that semantically close tokens (such as \"cat\" and \"dog\") have close associated encodings, but because $\\mathbb{N}$ is only one dimensional we would quickly face the limits of this technique: it would be impossible to capture the range of meanings of one word and how it compares to others in one dimension. This observation lead to the idea of embeddings.\n",
    "\n",
    "An embedding is the vector representation of a token in higher dimension. This dimension is set large enough to capture the meanings of a word in a higher context while keeping computations at a reasonable level\\footnote{Another trade-off we need to select.}. It gives a direct geometrical interpretation of how the tokens relate to each other. One can expect the vector representation of the word \"cat\" to be close in distance\\footnote{There are different ways to define said distance \\citep{word-distance, word2vec}.} to \"dog\" and both of them to be (relatively) far from \"peace\". We give an example of this representation in figure \\ref{embeddings_rep}. Yet, this semantic representation is highly dependent of the context, implying that embeddings usually need to be built with regard to a given corpus of texts. \n",
    "\n",
    "\\ \\\\\n",
    "We have now access to a mathematical semantically-meaningful token-separated representation of our text input. This input embedded matrix is up for further computations through the Transformer blocks. However, let's take a minute here. We remember that one of the advantages of the Transformer architecture is its natural parallel processing of tokens and that all computations are performed simultaneously. This implies that the model, conversely to the RNN architecture, has no natural indication on the tokens order. This is obviously a major shortcoming for the proper understanding and processing of the input since the word order in a sentence can be decisive to its meaning\\footnote{A quite eloquent example is provided in Appendix \\ref{sec:word-ordering}.}.\n",
    "\n",
    "When introducing the Transformer, \\cite{attention} mentioned and addressed this challenge. Their solution is to explicitly inform the model with the position of each token in the input sequence. To do that they originally proposed to include this positional information in the token embeddings that are passed to the Transformer blocks. Ideally, we would like to provide the information of the relative position between the tokens: the model could then leverage this by focusing more on the token's close dependencies and decreasing its attention over longer range dependencies \\citep{APE-issue}. Yet, encoding the relative position information is not straightforward. To circumvent this, they only encoded the absolute position of the tokens (i.e. the rank in the text input). As mentioned, their method includes this position encoding (PE) in the token embeddings, in other words, the positions are encoded as a $d$-dimensional vector which is then summed to the embeddings. Similarly as the token embeddings, this is a mapping from a non-negative integer (absolute position in the sequence) to a real-valued $d$-vector.\n",
    "\n",
    "\\ \\\\\n",
    " Their solution uses a combination of fixed-parameters sine and cosine functions with different frequencies for each token position. Specifically, they use the followings:\n",
    "\n",
    "This encoding, called Sinusoidal Position Encoding, is able to capture the token's absolute position thanks to its periodic nature. We provide further explanations with an intuitive representation in Appendix \\ref{sec:sinusoidalPE}. Additionally, they suspected that this set of functions and frequencies could attend the relative positions as well - this was later formally proven \\citep{sinusoidalPE-attendsRPE}.\n",
    "\n",
    "The authors also introduced an alternative to the Sinusoidal Position Encoding. They proposed along the Learned Position Embedding \\citep{attention, learned_PE2}. This method is easier to grasp as it follows the idea of token embedding. The original absolute position encodings (i.e. $1$, $2$, $3$, ...) are fed to an embedding layer of size $(\\texttt{embedding\\_dimension} \\times \\texttt{max\\_sequence\\_length})$. The embedding dimension is the same as the word embedding's so both higher-dimension representations can be added together. The parameters of this embedding layer are freely optimized along with the other layers of the model during training. Although philosophically simpler, the Learned Position Embedding naturally limits the context size. Indeed, once trained\\footnote{Once initialized to be exact.} for a given $\\texttt{max\\_sequence\\_length}$, the model is architecturally not able to process more tokens than this limit. Conversely, Sinusoidal PE can extrapolate with larger sequences than seen in training\\footnote{In fact, most PE methods have poor performance when it comes to extrapolation. Specific techniques had to be developed for extending the context window of a LLM. We quickly review some of these in section \\ref{sec:long-context}.}. In spite of this constraint, many major models have been designed with this embedding method (GPT-family \\citep{gpt-1, gpt2, gpt-3, santacoder}, BERT \\citep{bert}, ...).\n",
    "\n",
    "Nowadays, both of these Position Encodings are being progressively abandoned in favor of more efficient methods. Among the most common, we find the Rotary Position Embedding (RoPE) \\citep{rope} which re-use the original idea of Sinusoidal PE but directly injects the position information in the attention mechanism\\footnote{We present the attention mechanism this just after in section \\ref{sssec:transformer_blocks}.} with a matrix of rotation. Another common method is to inject in the attention mechanism the Relative PE (RPE) directly \\citep{original-RPE, alibi}. As mentioned above, this was the original intention given that the most important is to understand the position of a word in a given context rather than its absolute position.\n",
    "Although the research has been extremely active and has produced a large number of position encoding solutions, it is hard to spot a clear winner \\citep{nope}. They all have pros and cons and it is up to the architecture designer's best judgement to select one of them\\footnote{Let's just mention here some major Foundation Models and their PE method: GPT uses Learned PE \\citep{gpt-1}, LLaMA uses RoPE \\citep{llama} and T5 uses (an improved) RPE \\citep{t5}.}.\n",
    "\n",
    "\\ \\\\\n",
    "Whichever PE chosen, the embeddings are fed to the transformer blocks.\n",
    "\n",
    "\\subsubsection{Transformer Blocks}\n",
    "\n",
    "The second high-level part of a LLM is the actual Transformer\\footnote{This is an abusive name employed for the sake of simplicity here. \"Transformer\" originally designates one layer of an encoder or a decoder.}. This is where the magic happens, where the model learns to analyze tokens and produce new coherent ones. This is the actual NLU / NLG block. This architecture was the main contribution of \\cite{attention} which unlocked a new magnitude of performance in NL modelization. \n",
    "\n",
    "Right after comes the decoder whose role is to auto-regressively produce output tokens. It uses the encoder's output and the already-predicted output as inputs to generate the next token. To be precise, similarly as the encoder, it produces a vector in a latent space from which we can infer semantic value. It is then naturally oriented towards NLG tasks. Formally\\footnote{Using the prism of probability evoked earlier, we observe that the decoder is auto-regressively computing the probability $\\mathbb{P}_{\\theta_\\texttt{dec}}\\left( \\mathbf{y_i} | \\mathbf{\\widetilde X}_{dn}, \\mathbf{X'}_{d(i-1)} \\right)$. Using Bayes' rule we conclude that it can model the  whole target vector distribution \\citep{enc-dec-hfblog}.}, for a one-token output this writes\\footnote{Taking $d_\\texttt{enc}=d$.}:\n",
    "where $n$ is the number of tokens fed to the encoder, $m$ the number of tokens fed directly to the decoder, $d$ is the hidden size, $d_\\texttt{dec}$ is the dimension of the decoder latent space. We usually take $d_\\texttt{dec}$ to be equal to the hidden dimension $d$ as well.\n",
    "\n",
    "\\ \\\\\n",
    "It is possible to get rid of either the decoder or the encoder when adapted to the situation. Those models are called (resp.) encoder-only and decoder-only transformer models. The original encoder-decoder architecture is displayed in figure \\ref{fig:transfo_vaswani}.\n",
    "\n",
    "An encoder-only model is specialized in analyzing the semantics of a text and will be excellent for sentiment analysis or predicting a masked out token. Examples of encoder-only models would be BERT \\citep{bert} or RoBERTa \\citep{roberta}.\n",
    "\n",
    "Conversely, a decoder-only model is specialized in the generative area. It will output high quality chunks of text based on its input. When it is not coupled with an encoder, the only input is the previous text in the context it is working. Thus, the input can be empty\\footnote{We pass a \\texttt{beginning-of-sentence} token marker to get the model started.} or a simulated prior text, concatenated with the previously auto-generated tokens. A decoder-only model is generally employed for text generation tasks like question/answering or text summarization. Examples of decoder-only models would be LLaMA \\citep{llama}, GPT \\citep{gpt-3} or Falcon \\citep{falcon}.\n",
    "\n",
    "Finally, encoder-decoder models are used when the task requires a high level of both NLU and NLG. In the cases of language translation and speech-to-text transcription for instance, they can prove more efficient than encoder-only or decoder-only. Examples of encoder-decoder models would be BART \\citep{bart}, T5 \\citep{t5} or Whisper \\citep{whisper}.\n",
    "\n",
    "Let's now dive inside one block and get a technical feeling of how it works.\\footnote{The encoder and decoder block architectures being extremely close, we detail the architecture of a generic Transformer block and carefully flag whenever something differs between the two.} Essentially, the Transformer is composed of the Attention Mechanism (AM) and a Feed-Forward Network (FFN). It also features skip connections and normalization layers located - originally \\citep{attention} - right after them. In more recent works, the normalization layers have been moved before the AM and FFN \\citep{gpt2}. Other than that, it has remained the same throughout the years. \n",
    "\n",
    "\\ \\\\\n",
    "The Attention Mechanism is the core process of the model. It is the part modeling the dependencies between tokens and creating an attention score. To this end, the input embedded tokens are triply projected, via the use of linear layers, into the \\texttt{query} $\\mathbf{Q}$, \\texttt{key} $\\mathbf{K}$, and \\texttt{value} $\\mathbf{V}$ matrices\\footnote{The dimension of the attention matrices is usually the hidden dimension $d$.}. We can then compute the self-attention with the following formula:\n",
    "\\begin{equation*}\n",
    "    \\texttt{attention} \\left(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\right) = \\texttt{softmax}\\left( \\dfrac{\\mathbf{Q} \\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}\n",
    "\\end{equation*}\n",
    "where $d_k$ is the dimension of the \\texttt{keys}. This formula allows the tokens to attend all of each other and $1/\\sqrt{d_k}$ plays the role of a scaling factor.\n",
    "\n",
    "\n",
    "However this self-attention is a bit different for the decoder blocks. In this case, we want a given token to only be aware of what comes before itself (auto-regressive). If it were able to see in the \"future\" it would be cheating and lead to poor performance as in inference it would not be able to do so. Then we use an attention mask to mask out the future tokens after the first matrix multiplication (triangulation)\\footnote{We also distinguish the encoder and decoder blocks by using the nomenclatures \"bidirectional transformer\" and \"left-to-right transformer\".}. \n",
    "\n",
    "Moreover, in the case of a decoder block from an encoder-decoder model (and only in this case\\footnote{This is skipped in a decoder block from a decoder-only model.}), this self-attention is followed by a cross-attention layer. The principle is the same but allows to acknowledge the output of the prior encoder. The \\texttt{query} matrix in the cross-attention formula is still projected from the decoder's inputs but the \\texttt{keys} and \\texttt{values} are from the encoder's outputs. This allows the decoder tokens, in an encoder-decoder, to attend over all tokens in the input sequence processed through the encoder.\n",
    "\n",
    "\\ \\\\\n",
    "This was the description of how a single attention head works. To enhance the capability of the overall attention network, a model generally includes multiple heads that are run in parallel. They break the projection dimension in multiple blocks ($d_k = h \\times d_k$, where $h \\in \\mathbb{N}^*$ is the number of heads) to keep a similar calculation cost. Having multiple heads allows \"to jointly attend to information from different representation subspaces at different positions\" \\citep{attention}. Finally, these are concatenated and projected back to the hidden dimension. This very commonly used process is called Multi-Head Attention.\n",
    "\n",
    "\\ \\\\\n",
    "Although powerful, the AM has quadratic complexity in the sequence length, making it the bottleneck when it comes to scaling. This has been a major concern in the community and has been addressed by a large number of scientists. Many papers have tried to accelerate the AM process, sometimes at the cost of a reduced precision. Some techniques modify the attention to tend toward a linear complexity \\citep{sparse-attn, reformer-attn, long-attn, lin-attn}, other impact the multi-head architecture \\citep{multi-query, grouped-query}, or propose optimization in the calculations process \\citep{flash-attn1, flash-attn2}. Some of them are now extremely popular and widely used, even in the pre-training phase of Foundation Models \\citep{mistral}. \n",
    "\n",
    "\\ \\\\\n",
    "Still in the Transformer block, after the AM, the tensors are processed through a fully-connected Feed-Forward Network which is a Multi-Layer Perceptron (MLP) composed of two linear layers. The MLP is philosophically dedicated to \"making sense\" out of the attention analysis. This part of the model does not present any specificity so we don't extend on this.\n",
    "\n",
    "\\chapter{Background Work}\n",
    "\n",
    "In this chapter, we continue our theoretical journey of the study of LLMs. We introduce several machine learning techniques that can be applied to (but not only) LLMs to leverage their potential and ensure an optimal performance. They are diverse, providing architecture enhancements or dataset optimization, and form an excellent foundation knowledge to reach our final objective of building our own code translation LLM.\n",
    "\n",
    "\n",
    "\\section{Mixture-of-Experts}\n",
    "\n",
    "The Mixture-of-Experts (MoE) \\citep{moe} originates from the observation that to improve the capability of NNs, the common usage is to increase the size and number of parameters. However, doing so necessarily worsens the efficiency of the model by increasing the latency. The MoE proposes to solve this by expending the number of parameters while only using a small number when actually using the model. Most nodes end-up unused when an example is passed, which allows to maintain the original efficiency. The core idea lies in the hypothesis that the parameters will tend towards domain experts and will be activated only when an example of their knowledge domain is fed. \n",
    "\n",
    "\\ \\\\\n",
    "This design is formalized with a set of $N$ experts and a gating\\footnote{Also called routing in what follows.} network. The experts $E_i$ are not bound to share the same architecture but they tend to in the literature\\footnote{They all are 2-layer FFN in the original MoE paper.}. The only constraint is they should all require similar input shape and produce similar output shape. The gating network $\\mathbf{G}$ is usually a simple weight matrix whose second dimension is the number of experts. This layer selects a small number of experts $k\\leq N$\\footnote{We could use them all or a larger number but that would defeat the original purpose of the paper. The actual name is \"Sparsely-Gated MoE\".} to route to. And the input is sent to those experts. The final output of the MoE module is the weighted sum of the selected-expert' outputs.\n",
    "This formula leverages the capability of multiple experts and trains them on specific domain examples. This reduces the complexity of the problem each expert has to solve. We can expect to unlock a new magnitude of performance when compared to the original model.\n",
    "\n",
    "\\ \\\\\n",
    "Although quite simple, the MoE presents some delicate challenges related to the training phase. Firstly, we need to introduce the sparsity constraint in the $\\mathbf{G}$ formula. Secondly, training such a network raises the question of bias in the experts chosen. Indeed, during the first step, the network will select $k$ experts for the first time. This experts will see their weights updated during the optimization step. As they are the only experts updated at this point, they probably have a better performance than others and are more likely to be selected by the routing network in what follows. If we do nothing, then a vicious circle takes over and only a couple of experts will be trained and used, defeating the advantage of having $N$ experts. To counteract this, the authors introduce noise in the training phase which should balance the examples fed to the experts\\footnote{To be exhaustive, they also and mainly propose an additional loss to the model. This loss encourages all experts to have similar importance and acts via backpropagation.}. The gating network they use can be written as:\n",
    "\\begin{equation*}\n",
    "    \\mathbf{G}(\\mathbf{x}) = \\texttt{softmax} \\left( \\texttt{KeepTopK} \\left( \\mathbf{H}(\\mathbf{x}), k  \\right) \\right)\n",
    "\\end{equation*}\n",
    "with \n",
    "\\begin{equation*}\n",
    "    H(\\mathbf{x})_i = \\left( \\mathbf{x} \\cdot \\mathbf{W}_g \\right)_i + \\mathcal{N}_i \\cdot \\texttt{softplus} \\left( \\left( \\mathbf{x} \\cdot \\mathbf{W}_n \\right)_i \\right)\n",
    "\\end{equation*}\n",
    "and\n",
    "\\begin{equation*}\n",
    "    \\texttt{KeepTopK} \\left( \\mathbf{v}, k \\right)_i =\n",
    "    \\left\\{\n",
    "    \\begin{array}{ll}\n",
    "    v_i &\\quad \\text{if $v_i$ is among the top $k$ elements of $\\mathbf{v}$}\\\\\n",
    "    -\\infty &\\quad \\text{otherwise}\n",
    "    \\end{array}\n",
    "    \\right.\n",
    "\\end{equation*}\n",
    "where $\\mathbf{W}_g$ is the gating weight matrix, $\\mathbf{W}_n$ is a noise trainable matrix and $\\mathcal{N}_i$ is the random result of a draw from a normal distribution.\n",
    "\n",
    "In a nutshell, the MoE architecture comes from a simple idea and applies it in a natural way with the objective to unlock performance by augmenting the parameters without slowing the model. Previous papers have demonstrated its effectiveness in multiple contexts \\citep{demix-moe, paramefficient-moe}. Yet, some challenges regarding the training of such non-continuous and non-sequential network oblige a certain number of mathematical tricks, making the actual model and implementation relatively complex.\n",
    "\n",
    "\n",
    "\\section{Parameter Efficient Fine-Tuning}\n",
    "\n",
    "The first idea that comes to mind is to simply select a reduced number of parameters that we want to update and freeze the rest during training. If this is technically an easy-to-implement possibility, it can be difficult to guess which parameters to keep unfrozen for an optimal fine-tuning. Moreover, because of the repetitive nature of the LLM architecture (based on similar sequential blocks), if one wants to affect one part they will actually impact $N$ times the network ($N$ being the number of blocks). This partially defeats the reducing-the-parameters-to-train idea.\n",
    "\n",
    "A plethora of more elaborated and more efficient techniques have been developed (see figure \\ref{fig:survey_peft}). In this section, we present a couple of those that we tried and which unveiled potential. \n",
    "\\begin{figure}[ht]\n",
    "    \\centering\n",
    "    \\includegraphics[width=.8\n",
    "\\textwidth]{Photos/survey_peft.png}\n",
    "    \\caption[PEFT methods taxonomy]{PEFT methods taxonomy \\citep{survey_peft}.}\n",
    "    \\label{fig:survey_peft}\n",
    "\\end{figure}\n",
    "\n",
    "\n",
    "\\subsection{Adapters}\n",
    "\n",
    "An excellent alternative to selecting parameters in our network is to add a new set of parameters. In between layers or alongside, we can attach a small module with brand new parameters. This method is called \"Adapter\" and was first introduced in 2019 \\citep{adapter}.  They proved to work well for LLMs and further works have leveraged this idea. \\citep{llamaadapter} provides an efficient Adapter framework for LLaMA-based models.\n",
    "\n",
    "\\begin{figure}[ht]\n",
    "    \\centering\n",
    "    \\includegraphics[width=.7\\textwidth]{Photos/adapter.png}\n",
    "    \\caption[Example of Adapter architecture]{Example of Adapter architecture \\citep{adapter}.}\n",
    "    \\label{fig:adapter}\n",
    "\\end{figure}\n",
    "\n",
    "The Adapter method allows to control the number of added parameters\\footnote{Usually from as low as $0.1\\%$ to a couple of percents} as well as their location in the network. One can easily adapt the architecture of its module and the incorporation method (series vs. parallel). Using additional modules instead of updating the parameters of the network is a great feature to counter the catastrophic forgetting\\footnote{Catastrophic forgetting is a known issue of reckless fine-tuning: if not done properly, the change in the original parameters can lead to a decreased performance as the model may jump out of a local minimum and forget part of its pre-training data \\citep{catastrophic_forgetting}.}. They are also flexible and can be easily removed to revert to the original model. With an Adapter Fine-Tuning, sharing a new model can be done through sharing the Adapter module weights only\\footnote{And by letting the receiver know the base model used of course.}.\n",
    "\n",
    "On the other hand, Adapters present a couple of cons. They introduced newly initialized weights which need a training from scratch. Even if it's a small number, it may be more difficult to train from scratch than adjusting existing parameters. They come with a lot of flexibility, maybe too much. One needs to figure out the size, the architecture and the location of Adapters. It might take a while to figure out the perfect setup. Technically, it can be uneasy to change the architecture of the original model, and to share the fine-tuned model one also needs to share exactly how to reproduce it. And finally, they introduce a small latency as they come with new parameters.\n",
    "\n",
    "\\subsection{Low-Rank Adaptation}\n",
    "As of 2023, the most popular PEFT method is the Low-Rank Adaptation (LoRA) \\citep{lora}. This method is also a special case of the more general Adapter framework. When developing this method, the authors made the assumption that the weights update matrix $\\Delta \\mathbf{W}$ of a given weights matrix $\\mathbf{W}$ of a linear layer in a neural network is usually over-parameterized. According to this, the update weights matrix would actually have a low intrinsic rank, meaning it could be re-written as a multiplication of two lower-rank matrices. Thus, LoRA introduces this lower-rank representation of the matrix $\\Delta \\mathbf{W}$ and trains it while keeping the original dense matrix frozen.\n",
    "\n",
    "Precisely, here are the steps to implement the LoRA method for a given linear layer of matrix $\\mathbf{W} \\in \\mathbb{R}^{d\\times k}$:\n",
    "\\begin{enumerate}\n",
    "    \\item Choose a small rank $r\\in \\mathbb{N}^*$ that will represent the low-rank representation of the dense matrix, and set $\\alpha \\in \\mathbb{R}_{*}^{+}$ as the scaling factor.\n",
    "    \\item Introduce two matrices\\footnote{aka LoRA weights.} $\\mathbf{A}\\in \\mathbb{R}^{r\\times k}$ and $\\mathbf{B}\\in \\mathbb{R}^{d\\times r}$.\n",
    "    \\item Initialize $\\mathbf{A} \\sim \\mathcal{N}\\left(0, \\sigma^2 \\right)$ and $\\mathbf{B} = \\mathbf{0}$.\n",
    "    \\item Freeze $\\mathbf{W}$ (prevent weight updates).\n",
    "    \\item Compute the forward pass $\\mathbf{h}(\\mathbf{x}) = \\mathbf{W}\\mathbf{x} + \\dfrac{\\alpha}{r} \\cdot \\mathbf{BAx}$.\n",
    "    \\item Update $\\mathbf{A}$ and $\\mathbf{B}$ in the backward pass.\n",
    "\\end{enumerate}\n",
    "\n",
    "The forward pass is the result of passing the input to both the original matrix and the new ones in parallel, and adding the results together. This is visualizable in figure \\ref{fig:lora}. \n",
    "\n",
    "LoRA benefits from all the advantages of the Adapters framework but it has additional pros own to itself. It further reduces the number of parameters (low rank $r$) sometimes by a huge factor (up to a $10^4$ reduction). It also decreases the degrees of freedom to more tangible parameters ($r$, $\\alpha$, list of dense layers to LoRA-fy) in comparison to the quasi-infinite possibilities of the general Adapter. And the most important feature of LoRA is its nature that allows it to be merged with the original model: performing the update operation $\\mathbf{W} \\leftarrow \\mathbf{W} + \\dfrac{\\alpha}{r} \\cdot \\mathbf{BA}$ enables to remove the LoRA weights and avoid any computation overhead and latency, and to fall back to the original architecture. Then, one can choose to merge the LoRA weights after training or keep them in parallel to fit best their application.\n",
    "\n",
    "\\begin{figure}[ht]\n",
    "    \\centering\n",
    "    \\includegraphics[width=.7\\textwidth]{Photos/lora.pdf}\n",
    "    \\caption[LoRA's forward pass]{LoRA's forward pass. On the left, the LoRA module is not merged with the model. On the right, the LoRA module has been merged with the model.}\n",
    "    \\label{fig:lora}\n",
    "\\end{figure}\n",
    "\n",
    "As mentioned, LoRA is extremely famous because of these advantages and because, in some cases, it can match or outperform full fine-tuning's performance.\n",
    "\n",
    "\\subsection{Unified PEFT Framework}\n",
    "\\label{ssec:unified_peft}\n",
    "\n",
    "Before wrapping up this PEFT section, we just want to mention the recent efforts trying to unify all these methods. Huggingface built a PEFT library \\citep{peft} to gather these techniques under one framework that allows the users to seamlessly switch their fine-tuning method. Additionally, \\cite{unifiedpeft} mathematically proved an equivalence between multiple PEFT techniques (including the ones we discussed above) and regrouped them all under a unified architecture. \\cite{llmadapters} participated to this effort as well.\n",
    "\n",
    "\\ \\\\\n",
    "This last observation gives us a hint that combining these fine-tuning methods, even if orthogonal, may not bring consistent improvements over just using one.\n",
    "\n",
    "\n",
    "\\section{Curriculum Learning}\n",
    "Humans learn at school through a very organized and carefully designed curriculum. The education system has been evolving to optimize our understanding and learning efficiency\\footnote{Or at least it tries to...}. In general, a teacher will prone increasing the complexity over time to facilitate the grasping of new knowledge to their students. They will first introduce basic concepts alongside accessible examples so the students can conveniently use their prior knowledge to make sense out of this new information. Once they are familiar with the basics, the teacher will introduce new elements of increasing complexity. This way the students can progressively build their knowledge relying on their prior understanding. \n",
    "\n",
    "\\ \\\\\n",
    "Curriculum Learning (CL) \\citep{curriculum} emerged from this idea. The authors hypothesized that if humans learn progressively best, then it is also probably the case for neural networks\\footnote{Reinforcing the analogy with the human brain.}. CL is a machine learning technique that can be applied to any problem since it defines an order strategy on the dataset and has no impact on the model's architecture.\n",
    "\n",
    "The original CL paper proposes to define an ordering strategy on the dataset used to train the model. By showing first the \"easy\" examples and gradually increasing the complexity of examples fed, the model is expected to grasp concepts faster and more consistently. The authors came to this conclusion that a machine learning model trained with a CL strategy can achieve a better performance within less training steps. \n",
    "\n",
    "Overall, CL strategies improve consistently the convergence towards a local minimum for highly non-convex functions (such as the large neural networks functions) and work as a regularizer in the training process for a very low cost (when kept simple). \n",
    "\n",
    "\n",
    "\\chapter{SteloCoder}\n",
    "\n",
    "\\section{Motivation}\n",
    "\n",
    "A very common use case of Large Language models is coding assistants. LLMs can be of great help to workers, especially in the world of tech. Because of their natural ability to regroup knowledge in one place and because software engineers and machine learning engineers were the ones to develop such models in the first place, it only made sense for them to leverage LLMs in their everyday work. Thus, asking help from a LLM such as ChatGPT with a code snippet, either for example generation, description in natural language or translation to another Programming Language (PL) has become an extremely common action in tech companies. \n",
    "\n",
    "Of those use cases, one caught our attention. Translating a code snippet from a programming language to another one is crucial in some fields. It could help companies migrate their code from one technology to another seamlessly. This is vastly important as in some companies (and sometimes field-wide) old technology is still currently in use and need to be replaced. Yet, the technology has been abandoned for so long that only a few people are still knowledgeable with it, making the transition exceedingly tedious and expensive. A prime example of this is the coding language Cobol\\footnote{We give an example of Cobol snippet in Appendix \\ref{sec:cobol}.} which is the base foundation of many finance companies while being completely outdated and unknown to most engineers of today.\n",
    "\n",
    "\\ \\\\\n",
    "We propose to address this need. We are not the first to come up with the idea of building a Programming Language Model (PLM), many PLM already exist and were designed for different tasks with different objective and different capability and performance. Among those PLMs, some tend to be developed as Foundation Models so they can be used for any coding task (in-filling, translation, generation, NL explanation...) as a complete AI coding assistant. Other prefer to leverage them for a specific fine-tuning and enhance the capability of a model in a specific domain.\n",
    "\n",
    "Our project follows this last maxim and we aim to develop a translation-specialized PLM. Precisely, we aim to design, build and train a PLM to translate snippets (and potentially larger chunks) of code from any programming language to Python. We acknowledge that Python is one of the most known and used PL and imagine that it would be useful to programmers. We restrict ourselves this way, hoping that we can improve state-of-the-art models on this specific task.\n",
    "\n",
    "\n",
    "\\section{Architecture}\n",
    "\n",
    "\\subsection{Base Model: StarCoder}\n",
    "\n",
    "\n",
    "Using our knowledge of LLMs\\footnote{See chapter \\ref{chap:llm}}, we quickly figured out that an encoder-decoder model is adapted to the translation task. Indeed, the encoder unveil the complex and intrinsic dependencies of the tokens to map the original sentence to a high-dimensional representation of this information. The decoder can use this abstract representation to auto-regressively generate a corresponding sentence. Such models have actually been extensively used for translation tasks producing SOTA results \\citep{attention}.\n",
    "\n",
    "However, even if originally developed as encoder-decoder, transformer models have in majority first shifted to encoder-only and recently to decoder-only (see figure \\ref{fig:survey_llm}). This implies that the Foundational SOTA Models of today are almost exclusively decoder-only models. As we cannot ignore the advantage of an existing pre-trained model, we resign ourselves to selecting one from the decoder-only pool.\n",
    "\n",
    "\\ \\\\\n",
    "Finally, because we want to design a model that deals exclusively with programming language, we focus on models that have a very large part of their pre-training data to be code. In fact, we even note the existence of Foundation Models for programming language and decide these are our best bet. Among those, we select the SOTA model\\footnote{At the time...}: StarCoder \\citep{starcoder}.\n",
    "\n",
    "\\subsubsection{StarCoderBase \\& StarCoder}\n",
    "StarCoderBase \\citep{santacoder} is a Foundation Model built by the BigCode community which regroups a large number of engineers and organizations. It's a $15.5$B-parameter model, built on the BigCode architecture \\citep{santacoder} and trained on $1$T tokens of programming-related corpus and languages. This model is designed to understand and generate code from a large range of programming languages. It can fill-in code, explain an algorithm, answer coding questions, generate code in plenty of languages, create a documentation or comments,... In a word, it's a coding assistant.\n",
    "\n",
    "StarCoder is a fine-tuned model from StarCoderBase on a large Python dataset. This model has the same capabilities as the original one but is specialized for Python. This training process produces state-of-the-art results on Python code-related tasks \\citep{starcoder-sota}.\n",
    "\n",
    "\\ \\\\\n",
    "StarCoder's architecture is following the original Transformer decoder-only architecture with the following characteristics\\footnote{We focus on the characteristics presented in our previous chapters - which are not exhaustive.}:\n",
    "\\begin{itemize}\n",
    "    \\item Decoder blocks: $40$\n",
    "    \\item Vocabulary size: $49152$\n",
    "    \\item Max sequence length: $8192$\n",
    "    \\item Hidden size: $6144$\n",
    "    \\item MLP intermediate size: $24576$\n",
    "    \\item Attention Head: $48$\n",
    "    \\item Position encodings: Learned Position Encoding\\footnote{Which makes the max sequence length a hard constraint.}\n",
    "    \\item Attention Mechanism: Vanilla + Multi-Query\\footnote{\\cite{multi-query}.}\n",
    "    \\item Head: Causal Head\n",
    "\\end{itemize}\n",
    "\n",
    "\n",
    "\\subsection{SteloCoder's Little Twist}\n",
    "SteloCoder re-use the architecture of StarCoder as its backbone, but it's not its whole design. From the knowledge gathered prior to this work and partly presented in chapters \\ref{chap:introduction} and \\ref{chap:back_work}, we acknowledge that we cannot fine-tune the whole model. But also that having specialized modules enhances the capability of a model. Usually, it comes at a cost of additional parameters that may create latency and mitigate the possibility of a multi-task model. However, to circumvent this we propose to combine the LoRA and MoE techniques. \n",
    "\n",
    "\\ \\\\\n",
    "Our prior experiments led us to select LoRA as our fine-tuning method as it yields great performance along plenty of advantages evoked earlier. We actually decide to LoRA fine-tune the model one time for each language we want our model to handle. For each programming language, we get a set of LoRA weights (which are almost negligible in size in comparison to the backbone model) and store them. \n",
    "\n",
    "In the final model, the LoRA weights are kept on the side, on a parallel route to the spine, and only one set of parameters is selected during inference. The selected set should correspond to the PL passed as input. This way, we leverage the specialization of each LoRA module in a greater multi-language network at practically no latency or memory costs. In other words, we shift the MoE idea of multiple experts for improved precision on one task to multiple experts for multiple tasks.\n",
    "Another advantage of this design, is the possibility to easily add additional experts later. One could improve the model by adding support for more languages by training new experts without having to re-train (or re-fine-tune) the whole model. This also greatly reduces the risk of catastrophic forgetting.\n",
    "\n",
    "\\ \\\\\n",
    "LoRA is applied to three matrices in the attention mechanism (the linear layer producing the query, keys and values which is one matrix in BigCode implementation, the attention projection layer, and the MLP projection layer) within all $40$ transformer blocks.The LoRA weight are parametrized with $r = 4$ and $\\alpha = 32$. This sums up to an additional $9$M parameters per expert (around $0.06\\%$ of the total).\n",
    "\n",
    "\\ \\\\\n",
    "A major challenge of this design is the question of selecting the expert to use. We want the model to seamlessly choose the right expert. To achieve this we get inspiration from the MoE routing network. The original MoE gating has extra layers of complexity that we don't need in our case (especially because we want to select only one expert and because there are low risks of experts imbalance due to multitasking). We simplify and adapt the original architecture so it is reduced down to a linear layer coupled with a max pooling as shown in figure \\ref{fig:routing_path}. The linear layer projects the input to a dimension of size equal to the number of experts (i.e. equal to the number of PL handled) and then the max pooling aggregates the information along the sequence length dimension. The output is softmaxed to yield interpretable probability and we assume the index of max-probability to be the right expert to select: this index is propagated throughout the network\\footnote{Note that the final version is implemented at a batch-level, meaning we can pass multiple inputs from different PL as one batch and the model will select the right expert for each input of the batch. This is true for the training mode, and almost for the inference mode (see the GitHub repository). But usually, in inference, the batch size is $1$.}.\n",
    "\n",
    "\\section{XLCost Framework}\n",
    "\n",
    "\\subsection{XLCost Dataset}\n",
    "The data was gathered from the website GeeksForGeeks which proposes data structures and algorithms problems. It also hosts the solution in various PLs. Examples of such problems would be writing an algorithm to determine the first prime numbers or the first elements of the Fibonacci sequence. \n",
    "\n",
    "This dataset is available via GitHub and has been used as an evaluation framework for various models on various tasks. The results are displayed for open comparison. We discuss in the next subsection the important models and results for our application.\n",
    "\n",
    "\\ \\\\\n",
    "The XLCost dataset provides data for $7$ PLs: Python, C++, C\\#, C, JavaScript, Java and PHP. The amount of available data for each language is relatively balanced, except for C which presents only a few examples in comparison. To avoid issues later in training\\footnote{Some effort could be put into augmenting the data but this not our focus in this project.}, we decide to drop the C language from our scope. Table \\ref{table:data} sums up the available data.\n",
    "\n",
    "\n",
    "\\subsection{CodeBLEU Metric}\n",
    "\n",
    "A metric that is not perfect but was created with these thoughts is the CodeBLEU Score \\citep{codebleu}. It is derived from the BLEU (Bilingual Evaluation Understudy) Score metric \\citep{bleu} that itself is designed for comparing a machine-generated translation with a reference translation. It works from a complex formula mixing the n-gram precision and a penalty for the brevity of the generated text. It is now commonly used as it has displayed a high correlation with human judgment and provides an automatic low-cost evaluation. \n",
    "\n",
    "CodeBLEU extends this idea by applying the same principles to programming language generation tasks. Instead of comparing the generated code to a set of predefined references, CodeBLEU uses a combination of syntactic and semantic evaluation methods to assess the quality of the generated code which is more adapted in regards to the difficulties mentioned above. Interestingly, it can be interpreted as a percentage, with a\n",
    "balanced-weighted addition of BLEU, weighted BLEU, syntax match, and data-flow match.\n",
    "\n",
    "\\ \\\\\n",
    "That's why the CodeBLEU score is used to assess the quality of the outputs generated by the models evaluated on the XLCost test dataset. Models fine-tuned on the train set can be compared by computing their CodeBLEU score on the test set.\n",
    "\n",
    "\n",
    "\\subsection{Baseline Models}\n",
    "The XLCost framework also refers to a set of models that have been fine-tuned and tested on the datasets by the authors. They initialize a couple of models with the pre-trained weights and default configuration from the original papers. They use their dataset to test them on different downstream tasks, including PL translation. They evaluate them using the CodeBLEU score and record the results in tables. This creates a benchmark for each task in the XLCost framework and allows future experiments to test and compare other models with their benchmark.\n",
    "\n",
    "\\ \\\\\n",
    "The models tested are CodeBERT \\citep{codebert}, PLBART \\citep{plbart} and CodeT5 \\citep{codet5}. CodeBERT is inspired from BERT methodology, it's one of the very first PL models and it's able to generate text from multiple PLs. PLBART, which re-uses BART architecture, thrives in code summarization, generation and translation. CodeT5, the latest, is established based on T5 architecture and provides state-of-the-art results in both code understanding and generation on various tasks.\n",
    "\n",
    "All three of them have been tested on the XLCosts tasks. The results are available on the GitHub page. We provide their CodeBLUE score for the PL-to-Python translation task in tables \\ref{table:baseline-snippet} and \\ref{table:baseline-program}. CodeT5 performs the best for all PL-to-Python translations in the XLCost framework. \n",
    "\n",
    "Note that this baseline models were trained and tested for all tasks in the XLCost framework, including PL translation to other languages than Python. We provide in Appendix \\ref{sec:codeblue} more results on these translation tasks.\n",
    "\n",
    "\n",
    "\\section{Training Process}\n",
    "Now that we have the model, the data, the evaluation metric and some baselines to compare to, we can focus on the actual training. \n",
    "\n",
    "\\ \\\\\n",
    "Given the specific architecture that we have developed for SteloCoder, we decide that having a two-step training is the best thing to do. Indeed, we will first train each language individually as this corresponds to a regular LoRA fine-tuning of StarCoder. This will produce the sets\\footnote{There will be $5$ sets of weights in total as this is the number of PLs our model will handle in the translation to Python task (C++, C\\#, JavaScript, Java, PHP).} of experts that will be kept on the side and route towards to in the final model. In a second and final step, we will train the routing gate - which as a reminder is only a linear layer - to choose the right now-trained experts to route the examples to.\n",
    "\n",
    "\\ \\\\\n",
    "This process breakdown is possible given the separation of the tasks and we expect it to stabilize and facilitate the global training of the network.\n",
    "\n",
    "\n",
    "\\subsection{Phase 1: Experts Training}\n",
    "The first phase is dedicated to training the sets of experts one by one in regards to their task. That is to say, we fine-tune StarCoder with the LoRA technique to translate code from C++ to Python. Then, we fine-tune again the original StarCoder to translate C\\# to Python. And so on for all $5$ PLs. Once a set of weights is trained it is stored as it is to be re-used in the second phase.\n",
    "\n",
    "During this phase, all parameters are frozen except the LoRA weights.\n",
    "\n",
    "\\ \\\\\n",
    "We adopt the self-supervised strategy as foreshadowed in chapter \\ref{chap:llm}\\footnote{We did not extend on this but there are obviously multiple training strategies according to the task and architecture of the model.}. The model receives as input the code in the original PL along with the concatenated Python translation. The reference label (to predict) for the model is then the same string: the model learns to predict the future tokens by judiciously adding a beginning-of-code marker token in the concatenation. We teach the model to predict the tokens following the beginning-of-code marker token\\footnote{This is not the usual method in translation as we tend to use encoder-decoder models that can adopt different training strategies.}. Precisely, the input (and label) follows this structure: \\texttt{<PL name> code in the given PL <py> Python code translation <|end-of-text|>}. In inference, we will provide the string \\texttt{<PL name> code in the given PL <py>} and the model will predict the Python translation tokens until the end-of-text token, which signals the end of the task.\n",
    "\n",
    "\\subsection{Phase 2: Gate Training}\n",
    "Once all the experts are trained and ready to use, we can gather them and prepare the final architecture. The experts need to be routed to via the routing gate. This MoE gate is the last piece we need to train. In this phase, these are the only parameters that are updated, everything else is frozen including the experts.\n",
    "\n",
    "\\ \\\\\n",
    "The MoE gate training rose a challenge linked to the use of the maximum probability. As a reminder, the output of this linear layer is softmaxed to create a probability-like output. We can then select the max-probability index as the expert to route to. However, the \\texttt{argmax} operation is not differentiable which prevents a backpropagation. A solution to this is to adopt slightly different behaviors in training and inference. During training, instead of using only the max-probability expert, we use the output of the softmax as weights. We pass the input through all the experts and calculate the weighted average of the outputs as our final expert output. This enables the backward propagation. \n",
    "\n",
    "Over training, the model encourages greater weight on the right expert, and during inference only this one will be used. \\cite{training-routing} proposes a similar solution as ours for training the gate.\n",
    "\n",
    "\\section{Results}\n",
    "\n",
    "\\subsection{Individual Experts}\n",
    "We test the experts one by one to assess the quality of the generated translation when the right expert is used (at the end of phase $1$). We use the XLCost framework for fair comparison with previous work. We then use the XLCost test datasets and the CodeBLUE score as metric. Each expert is tested solely for the language it was trained for.\n",
    "\n",
    "\\ \\\\\n",
    "A recurrent issue we spot in the code generation is the model's tendency to repeat a character, a word or a group of words an infinite number of times (until maximum generation reached)\\footnote{This suggests that a better text generation sampling method could probably improve the model's behavior.}. We also note the inconsistent performance when reproducing some common functions syntax (linked to the \\texttt{main} function) which, after a closer look, we notice to be missing from the snippet-level data (only present in the program-level data).\n",
    "\n",
    "\\subsection{Routing Gate}\n",
    "We measure the quality of our routing process by recording the accuracy of the expert-choosing task. On the XLCost balanced validation dataset, the routing process reached an accuracy of $91\\%$. This is a pretty good result. We provide a breakdown analysis of chosen experts in table \\ref{table:routing_accuracy}.\n",
    "\n",
    "\\begin{table}[ht]\n",
    "\\centering\n",
    "\\begin{tabular}{cccccc}\n",
    "           & C++ & PHP & JavaScript & C\\# & Java \\\\ \\cline{2-6} \n",
    "C++        & 149 & 0   & 1          & 0   & 3    \\\\\n",
    "PHP        & 1   & 152 & 0          & 0   & 0    \\\\\n",
    "JavaScript & 1   & 0   & 145        & 0   & 0    \\\\\n",
    "C\\#        & 2   & 1   & 7          & 150 & 47   \\\\\n",
    "Java       & 0   & 0   & 0          & 3   & 103  \\\\ \\toprule\n",
    "Total      & 153 & 153 & 153        & 153 & 153 \n",
    "\\end{tabular}\n",
    "\\caption[Experts-routing confusion matrix]{Validation Confusion Matrix. The predictions are on the left (rows), the true labels are above (columns). The global accuracy is $91.3\\%$.}\n",
    "\\label{table:routing_accuracy}\n",
    "\\end{table}\n",
    "\n",
    "\\ \\\\\n",
    "Analyzing the experts chosen suggests that the routing struggles the most for languages that have similar syntaxes such as C\\# and Java. We assume that longer context could solve at least partially the issue. Nevertheless, we also point out that this is far from being a major issue and even might be and advantage: if the languages are close enough then the associated experts will have no issue translating one language or the other regardless of which one it was trained for, the routing can then select the expert that will perform the best for the given specific input (increases the degrees of freedom for these languages).\n",
    "This last observation let us assume that the final model should present equivalent or better result than individual experts in table \\ref{table:experts-results}.\n",
    "\n",
    "\n",
    "\\subsection{SteloCoder}\n",
    "We now test SteloCoder on test cases from the XLCoST framework and display the results in table \\ref{table:stelocoder_results}. Except for the PHP language, SteloCoder beats the XLCost benchmark in terms of CodeBLUE score. We interpret this as two sided: on the one hand we have trained specialized expert for each specific task enhancing the capability of the network on each one of them, on the other we believe that StarCoder's original ability to produce qualitative Python code made such high scores possible.\n",
    "\n",
    "\\ \\\\\n",
    "We also compare the final model output against the original single-expert fine-tuned models outputs and observe that the routing network do not harm the performance. In some cases we even observed a slight improvement as suggested above. \n",
    "\n",
    "\\ \\\\\n",
    "We refer to Appendix \\ref{ssec:translation_examples} for samples of translation performed by SteloCoder.\n",
    "\n",
    "\n",
    "\\chapter{Limitations \\& Future Work}\n",
    "\n",
    "Building this project was extremely informative as it was only possible by first acquiring a somewhat deep and solid knowledge on multiple aspects of the subject. We are very satisfied with the results, as much as we are with the journey that got us here.\n",
    "\n",
    "\\ \\\\\n",
    "But our journey does not end just yet. Analyzing the results, and putting them in regards to the design choices we made is important to understand the impact of each one of them. In this chapter we propose an overview of a couple of limitations we identified in our work, along with several aspects that could be the object of further study.\n",
    "\n",
    "\\section{Limitations}\n",
    "\n",
    "\\subsection{Model Weaknesses}\n",
    "\\subsubsection{Post-Processing Pipeline}\n",
    "SteloCoder looks very good when it comes to code translation. However, we kept under the hood an aspect of the translation pipeline. There is an important question around the data formatting that is fed to the model and how it should be designed to come out right of the model. This design question has to be answered before the training since the model will follow exactly the training labels. This requires a precise and careful pre-processing of the data.\n",
    "\n",
    "\\ \\\\\n",
    "In the case of SteloCoder, we made several poor design choices and went over the pre-processing part (too) quickly. This resulted in a generation format that is not consistent, especially for spaces. Sometimes the model will produce double spaces for no solid reason for instance. Most of the time, this is not a major issue as Python can still be interpreted easily.\n",
    "However, in our case, because we evaluate the performance using an automated metric we need the format to be consistent and correct. Especially with the CodeBLUE score, the results can greatly vary because of syntax, format or style discrepancies. \n",
    "\n",
    "\\ \\\\\n",
    "To address this, and make our evaluation consistent with the other models tested in the benchmark, we added a post-processing pipeline to re-format the generated code to the right format. This is a similar add-on as done by \\cite{whisper}.\n",
    "Among other, the pipeline handles the spaces issue, removes add-on tokens (for PL name, Python and end-of-text), and re-format \\texttt{NEW\\_LINE}, \\texttt{INDENT} and \\texttt{DEDENT} tokens.\n",
    "\n",
    "\n",
    "\\subsubsection{Task Generalization}\n",
    "\\label{sssec:task_generalization}\n",
    "Another limitation of SteloCoder is its task-specific knowledge. SteloCoder was not designed to be foundational at all. As it can produce high quality code translation, one could expect the same model to also be efficient in other code-related task such as code generation or commenting code. \n",
    "\n",
    "\\ \\\\\n",
    "Other models (including those of the benchmark) have been showing great results in code translation while maintaining a good understanding and generation in other tasks \\citep{codet5, codebert, plbart}. Even though SteloCoder is still able to perform Python code generation tasks (as shown in Appendix \\ref{ssec:generationn_examples}), it remains highly limited, and it does not benefit from any broader knowledge due to StarCoder being already a code-specific model.\n",
    "\n",
    "\n",
    "\\subsection{Architecture Weaknesses}\n",
    "\n",
    "\\subsubsection{Flexibility}\n",
    "The design proposed with multiple experts and a routing gate is in fact very constraining. If adding an expert for extending the capability of the model is theoretically possible, it actually implies that in addition to training the expert, one also needs to re-train the routing gate from scratch. Possible, but not straightforward. \n",
    "Training the gate requires a balanced dataset which adds a layer of complexity when putting up together the dataset: each task needs a large enough dataset and if one has noticeably fewer examples it might need to be removed (similarly as how we ignored the C language in our application).\n",
    "\n",
    "\\subsubsection{Inefficient Training}\n",
    "The MoE gate had us introduce a different behavior in training to enable the backpropagation by using differentiable operations. Doing so, in training, we used a weighted average of all experts outputs instead of only taking the one of maximum-probability. This means that instead of passing the training sample only to one expert, we fed it to all experts before aggregating the results - which means a larger computational cost\\footnote{We mitigate this observation by mentioning that the cost of passing an example through an expert has a relatively low cost in regards to the rest of the model's computations.}.\n",
    "\n",
    "\\subsubsection{Inefficient Inference}\n",
    "The final LoRA logistics chosen here - keeping an unmerged parallel route - is not computationally optimal during inference. This design requires three matrix multiplications when passing through a LoRA-decomposed matrix. As shown in figure \\ref{fig:lora}, the input is multiplied with the original weights matrix $\\mathbf{W}$ and is also simultaneously multiplied with LoRA matrices $\\mathbf{A}$ and then $\\mathbf{B}$. A computationally efficient alternative is to introduce the new matrix $\\mathbf{C} = \\mathbf{B} \\times \\mathbf{A}$, computed only once. Whenever an input is going through a LoRA-decomposed matrix, one can first merge $\\mathbf{C}$ in $\\mathbf{W}$ and perform the matrix multiplication. Afterwards, the $\\mathbf{C}$ matrix is unmerged from $\\mathbf{W}$ using a simple subtraction. This requires only one matrix multiplication and is achievable at a (very limited) cost of memory efficiency.\n",
    "\n",
    "\n",
    "\\section{Broader General Knowledge}\n",
    "\n",
    "As mentioned in the limitations section \\ref{sssec:task_generalization}, one flaw of SteloCoder is that its base model, StarCoder, originates itself from a PL Foundation Model rather than a general Foundation Model. In other words, StarCoderBase was pre-train exclusively on code-related data. While this can be considered as an advantage for most use-cases (StarCoder-Base should be exceptionally strong in code-related tasks for a comparatively shorter training than other general Foundation Models), it can also become a constraining factor. \n",
    "\n",
    "A general-purpose Foundation Model has to be pre-train on a humongous large range of data. The model will be fed samples from every field, every domain, in plenty of formats. Regardless, such a model is built for any task so it has to know everything. Hence, once fine-tuned for a downstream application, it can still re-use this wide knowledge\\footnote{Modulo the catastrophic forgetting...} to improve its behavior and overall provide better quality outputs. StarCoder, and by extension SteloCoder, cannot take advantage of this. This is not negligible as reasoning learned through general data could help design algorithms and conversely the analysis of an unknown program could be done via the use of external knowledge.\n",
    "\n",
    "A possible fix to this issue, that we leave for future work, is to replace the base model with another one. This new base model should still be a code-specialized (if possible even a Python-specialized) Foundation Model, but also resulting from a fine-tuning process of a general-purpose Foundation Model. \n",
    "\n",
    "\\ \\\\\n",
    "The recent\\footnote{Comparatively to the start of this project.} releases of Code Llama and Code Llama - Python \\citep{code_llama}, both fine-tuned from LLaMA-2 \\citep{llama2}, which are new SOTA in code-related tasks, encourages us to think that a similar task-specific architecture applied to a PLM fine-tuned from a general-purpose LLM (such as Code LLaMA or Code LLaMA - Python) could yield excellent results while offering a broader general knowledge. \n",
    "\n",
    "\n",
    "\\section{Conclusion}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfc50a39cdcc49f0a60b95a5114e2233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizer class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51a721cc66a4460e915d3299a5c6556f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0f853469ab640e68cee81732b5989ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac4035da1824f2cad1cc3ce33518641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/12000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset1 = load_dataset('stingning/ultrachat')['train']\n",
    "dataset1 = dataset1.shuffle(42).select(torch.arange(30_000))\n",
    "dataset1 = dataset1.map(lambda x: build_chat(x, tokenizer))\n",
    "dataset1 = dataset1.filter(lambda x: len(x['input_ids']) <= 32_768)\n",
    "\n",
    "dataset2 = load_dataset('Yukang/LongAlpaca-12k')['train']\n",
    "dataset2 = dataset2.map(lambda x: build_qa_inputs(x, tokenizer))\n",
    "dataset2 = dataset2.filter(lambda x: len(x['input_ids']) <= 32_768)\n",
    "dataset2 = dataset2.shuffle(seed=42)\n",
    "\n",
    "dataset = concatenate_datasets([dataset1, dataset2]).shuffle(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chat(sample, tokenizer):\n",
    "    prompt = [{'role': 'user' if i % 2 == 0 else 'assistant', 'content': data} for i, data in enumerate(sample['data'])]\n",
    "    prompt = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
    "    inputs = tokenizer(prompt, add_special_tokens=False)\n",
    "    return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': inputs['input_ids']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_qa_inputs(sample, tokenizer):\n",
    "    prompt = [{'role': 'user', 'content': sample['instruction']}, {'role': 'assistant', 'content': sample['output']}]\n",
    "    prompt = tokenizer.apply_chat_template(prompt, tokenize=False)\n",
    "    inputs = tokenizer(prompt, add_special_tokens=False)\n",
    "    return {'input_ids': inputs['input_ids'], 'attention_mask': inputs['attention_mask'], 'labels': inputs['input_ids']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = split_dataset(dataset, 0.9)\n",
    "datasets = DatasetDict({\n",
    "        'train': train_dataset,\n",
    "        'val': val_dataset\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sizes = [len(x) for x in datasets['train']['input_ids']]\n",
    "plt.hist(sizes, bins=100);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context_extension",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
